{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code snippet is to interaction numpy and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_by(dataset, by):\n",
    "    \n",
    "    dataset = dataset.sort_values(by=[by])        \n",
    "    dataset = dataset.set_index(np.arange(dataset.shape[0]))   \n",
    "    \n",
    "    return dataset    \n",
    "\n",
    "def read_drop_duplicate_sort_by(path_to_file, file_name, by=''):\n",
    "    \n",
    "    dataset = pd.read_csv(path_to_file + file_name)\n",
    "    dataset = dataset.drop_duplicates()\n",
    "    dataset = dataset.dropna(axis=0, how='any')\n",
    "    \n",
    "    if by != '':\n",
    "        dataset = sort_by(dataset, by)        \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_file = 'D:\\\\computer_science\\\\'    \n",
    "    \n",
    "nodes_train = read_drop_duplicate_sort_by(path_to_file, 'nodes_train.csv',     by='Node')\n",
    "nodes_train.to_csv('nodes_train.csv', index=False)\n",
    "\n",
    "nodes_test  = read_drop_duplicate_sort_by(path_to_file, 'nodes_test.csv', by='Node')\n",
    "nodes_test  = nodes_test[96:]\n",
    "nodes_test  = nodes_test.set_index(np.arange(nodes_test.shape[0])) \n",
    "nodes_test.to_csv('nodes_test.csv', index=False)\n",
    "\n",
    "edges       = read_drop_duplicate_sort_by(path_to_file, 'edges_orig.csv')\n",
    "edges       = sort_by(edges, by='NodeLeft')\n",
    "edges.to_csv('edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many classes are in dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = nodes_train['Y'].drop_duplicates()\n",
    "\n",
    "print('Count of class labels is %d' % class_labels.shape[0])\n",
    "print('Max class label is %d' % class_labels.max())\n",
    "print('Min class label is %d' % class_labels.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feat1  = edges['Feat1'].max()\n",
    "min_feat1  = edges['Feat1'].min()\n",
    "mean_feat1 = edges['Feat1'].mean()\n",
    "\n",
    "print(max_feat1, min_feat1, mean_feat1)\n",
    "\n",
    "max_feat2 = edges['Feat2'].max()\n",
    "min_feat2 = edges['Feat2'].min()\n",
    "mean_feat2 = edges['Feat2'].mean()\n",
    "\n",
    "print(max_feat2, min_feat2, mean_feat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset_train(edges, nodes, num_rows=edges.shape[0]):        \n",
    "        \n",
    "    rows = edges.shape[0]\n",
    "        \n",
    "    dataset = pd.DataFrame(index=np.arange(num_rows), columns=['Node', 'Feat1', 'Feat2', 'Feat3', 'Feat4', 'Y'])    \n",
    "                                           \n",
    "    i, start = 0, time.time()\n",
    "    \n",
    "    count_equ = 0\n",
    "                        \n",
    "    for row, (node_train, node_test, feat1, feat2) in enumerate(zip(edges.NodeLeft, edges.NodeRight, edges.Feat1, edges.Feat2)):\n",
    "            \n",
    "        left  = nodes['Node'].searchsorted(node_train, side='left' )[0]\n",
    "        right = nodes['Node'].searchsorted(node_train, side='right')[0]\n",
    "                                           \n",
    "        for ind in range(left, right):                            \n",
    "\n",
    "            dataset.set_value(i, 'Node',  node_train)\n",
    "            dataset.set_value(i, 'Feat1', feat1)\n",
    "            dataset.set_value(i, 'Feat2', feat2)\n",
    "            dataset.set_value(i, 'Feat3', nodes.at[ind, 'Feat3'])\n",
    "            dataset.set_value(i, 'Feat4', nodes.at[ind, 'Feat4'])\n",
    "            dataset.set_value(i, 'Y',     nodes.at[ind, 'Y'])\n",
    "\n",
    "            i += 1       \n",
    "                                     \n",
    "        print('Search train node features %.1f %% ...' % (row/rows*100))\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print('Find %d train nodes.' % i)\n",
    "    \n",
    "    dataset = dataset.dropna(axis=0, how='any')\n",
    "    dataset = sort_by(dataset, by='Node')\n",
    "    \n",
    "    file_name, path_to_save = 'dataset_train.csv', os.getcwd()\n",
    "    \n",
    "    print('%s is created by %d sec' % (file_name, end - start))\n",
    "    \n",
    "    dataset.to_csv('.\\\\' + file_name, index=False)\n",
    "    \n",
    "    print('%s is saved to %s' % (file_name, path_to_save))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.isfile('data_train.csv'):\n",
    "make_dataset_train(edges, nodes_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset_test(edges, nodes_train, nodes_test, num_rows=edges.shape[0]):        \n",
    "        \n",
    "    rows = edges.shape[0]\n",
    "        \n",
    "    dataset = pd.DataFrame(index=np.arange(num_rows), columns=['Node', 'Feat1', 'Feat2', 'Feat4'])    \n",
    "                                           \n",
    "    i, start = 0, time.time()\n",
    "                        \n",
    "    for row, (node_train, node_test, feat1, feat2) in enumerate(zip(edges.NodeLeft, edges.NodeRight, edges.Feat1, edges.Feat2)):\n",
    "        \n",
    "        left1  = nodes_test['Node'].searchsorted(node_train, side='left' )[0]\n",
    "        right1 = nodes_test['Node'].searchsorted(node_train, side='right')[0]        \n",
    "         \n",
    "        for ind in range(left1, right1):  \n",
    "                \n",
    "            dataset.set_value(i, 'Node',  node_train)\n",
    "            dataset.set_value(i, 'Feat1', feat1)\n",
    "            dataset.set_value(i, 'Feat2', feat2)\n",
    "            dataset.set_value(i, 'Feat4', nodes_test.at[ind, 'Feat4'])\n",
    "\n",
    "            i += 1       \n",
    "                                           \n",
    "        left2  = nodes_test['Node'].searchsorted(node_test, side='left' )[0]\n",
    "        right2 = nodes_test['Node'].searchsorted(node_test, side='right')[0]        \n",
    "                     \n",
    "        for ind in range(left2, right2):  \n",
    "                \n",
    "            dataset.set_value(i, 'Node',  node_test)\n",
    "            dataset.set_value(i, 'Feat1', feat1)\n",
    "            dataset.set_value(i, 'Feat2', feat2)\n",
    "            dataset.set_value(i, 'Feat4', nodes_test.at[ind, 'Feat4'])\n",
    "\n",
    "            i += 1       \n",
    "                                   \n",
    "        print('Search test node features %.1f %% ...' % (row/rows*100))\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print('Find %d test nodes.' % i)\n",
    "    \n",
    "    dataset = dataset.dropna(axis=0, how='any')\n",
    "    dataset = sort_by(dataset, by='Node')\n",
    "    \n",
    "    file_name, path_to_save = 'dataset_test.csv', os.getcwd()\n",
    "    \n",
    "    print('%s is created by %d sec' % (file_name, end - start))\n",
    "    \n",
    "    dataset.to_csv('.\\\\' + file_name, index=False)\n",
    "    \n",
    "    print('%s is saved to %s' % (file_name, path_to_save))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset_test(edges, nodes_train, nodes_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check intersection between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_test_left = pd.Index(nodes_test['Node']).intersection(pd.Index(edges['NodeLeft']))\n",
    "print(nodes_test_left.shape)\n",
    "\n",
    "nodes_test_right = pd.Index(nodes_test['Node']).intersection(pd.Index(edges['NodeRight']))\n",
    "print(nodes_test_right.shape)\n",
    "\n",
    "nodes_test_left_train  = pd.Index(nodes_train['Node']).intersection(nodes_test_left)\n",
    "nodes_test_right_train = pd.Index(nodes_train['Node']).intersection(nodes_test_right)\n",
    "\n",
    "print(nodes_test_left_train, nodes_test_right_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
